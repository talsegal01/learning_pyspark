{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# DO NOT RUN THIS NOTEBOOK LOCALLY\n", "## This notebook is set up to run on Google dataproc"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# wk8 Demo - Advanced Spark - Pipelines and Optimizations with DataFrames\n", "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2019`__\n", "\n", "So far we've been using Spark's low level APIs. In particular, we've been using the RDD (Resilient Distiributed Datasets) API to implement Machine Learning algorithms from scratch. This week we're going to take a look at how Spark is used in a production setting. We'll look at DataFrames, SQL, and UDFs (User Defined Functions).  As discussed previously, we still need to understand the internals of Spark and MapReduce in general to write efficient and scalable code.\n", "\n", "In class today we'll get some practice working with larger data sets in Spark. We'll start with an introduction to efficiently storing data and approach a large dataset for analysis. After that we'll discuss a ranking problem which was covered in Chapter 6 of the High Performance Spark book and how we can apply that to our problem. We'll follow up with a discussion on things that could be done to make this more effiicent.\n", "* ... __describe__ differences between data serialization formats.\n", "* ... __choose__ a data serialization format based on use case.\n", "* ... __change__ submission arguements for a `SparkSession`.\n", "* ... __set__ custom configuration for a `SparkSession`.\n", "* ... __describe__ and __create__ a data pipeline for analysis.\n", "* ... __use__ a user defined function (UDF).\n", "* ... __understand__ feature engineering and aggregations in Spark.\n", "\n", "__`Additional Resources:`__ Writing performant code in Spark requires a lot of thought. Holden's High Performance Spark book covers this topic very well. In addition, Spark - The Definitive Guide, by Bill Chambers and Matei Zaharia, provides some recent developments."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notebook Set-Up"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is running on dataproc with the following setup\n", "\n", "```{bash}\n", "BUCKET=\"w261-bucket\"\n", "CLUSTER=\"w261-demo\"\n", "PROJECT=\"w261-216504\"\n", "JUPYTER_PORT=\"8123\"\n", "PORT=\"10000\"\n", "ZONE=$(gcloud config get-value compute/zone)\n", "\n", "# CREATE DATAPROC CLUSTER\n", "gcloud dataproc clusters create ${CLUSTER} \\\n", "    --metadata \"JUPYTER_PORT=${JUPYTER_PORT}\" \\\n", "    --metadata \"JUPYTER_CONDA_PACKAGES=numpy:pandas:scipy:pyarrow\" \\\n", "    --metadata \"JUPYTER_CONDA_CHANNELS=conda-forge\" \\\n", "    --project ${PROJECT} \\\n", "    --bucket ${BUCKET} \\\n", "    --image-version \"1.3.10-deb9\" \\\n", "    --initialization-actions \\\n", "       gs://dataproc-initialization-actions/jupyter/jupyter.sh \\\n", "    --num-preemptible-workers=4 \\\n", "    --num-workers=2 \\\n", "    --worker-machine-type=n1-standard-8 \\\n", "    --master-machine-type=n1-standard-8\n", "    \n", "# CREATE SOCKS PROXY\n", "gcloud compute ssh ${CLUSTER}-m \\\n", "    --project=${PROJECT} \\\n", "    --zone=${ZONE}  \\\n", "    --ssh-flag=\"-D\" \\\n", "    --ssh-flag=${PORT} \\\n", "    --ssh-flag=\"-N\"\n", "\n", "```\n", "--------"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "# USE SOCKS PROXY (if your local computer is linux)\n", "/usr/bin/google-chrome \\\n", "  --proxy-server=\"socks5://localhost:${PORT}\" \\\n", "  --user-data-dir=/tmp/${CLUSTER}-m   \n", "\n", "# USE SOCKS PROXY (if your local computer is a mac)\n", "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n", "  --proxy-server=\"socks5://localhost:${PORT}\" \\\n", "  --user-data-dir=/tmp/${CLUSTER}-m  \n", "\n", "# MORE INFO ON SETTING UP SOCKS PROXIES IN GCP:\n", "# https://cloud.google.com/solutions/connecting-securely#socks-proxy-over-ssh\n", "```"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# imports\n", "import re\n", "import os\n", "import numpy as np\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Load the data\n", "Today we'll be using GSOD weather station data, avaliable from Google in BigQuery.\n", "\n", "Since this is a decent sized dataset (21 GB uncompressed) we won't be running code, but rather reviewing the process."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get data from BigQuery into Google Cloud Storage as GZIP compressed CSV files\n", "!bq --location=US extract --compression GZIP 'bigquery-public-data:samples.gsod' gs://w261-bucket/gsod/gsod-*.csv.gz\n", "!bq --location=US extract --compression GZIP 'bigquery-public-data:noaa_gsod.stations' gs://w261-bucket/gsod/stations.csv.gz"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Initialize Spark"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["spark"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Here we show how to do a custom configuration\n", "sc = spark.sparkContext"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import SQLContext\n", "sqlContext = SQLContext(sc)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sc.getConf().getAll()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 1. Structured API - Datasets, DataFrames, and SQL Tables and Views\n", "\n", "A Dataset is a distributed collection of data. Datasets provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL\u2019s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python\u2019s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n", "\n", "A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n", "    \n", "This makes the analysis of data similar to how we would do analysis with Python's Pandas or R's dataframes. Spark DataFrames are heavily inspired by Pandas and we're actually able to create Pandas user-defined functions (UDFs) to use with Spark which leverage the Apache Arrow project to vectorized computation instead of row-by-row operations. This can lead to significant performance boosts for large datasets. \n", "    \n", "SQL Tables and Views are basically the same thing as DataFrames. We simply just execute SQL against them instead of DataFrame code  *(Defintive Guide, pg. 50)*. You can choose to express some of your data manipulations in SQL and others in DataFrames and they will compile to the same underlying code. *(Defintive Guide, pg. 179)*"]}, {"cell_type": "markdown", "metadata": {}, "source": [" > __DISCUSSION QUESTIONS:__ \n", " * _Why would we want to use RDDs in this class over DataFrames?_\n", " * _What is a UDF? Why do we need to create them?_\n", " * _What is vectorized computation and how does that differ from row-by-row function calls_\n", " * _How is a Dataset different than a DataFrame?_\n", " * _Are Datasets avaliable in the Python API?_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 2. Data Serialization Formats. \n", "This week you read [Format Wars](http://www.svds.com/dataformats/) which covered the characteristics, structure, and differences between raw text, sequence, Avro, Parquet, and ORC data serializations. \n", "\n", "There were several points discussed: \n", "\n", "* Human Readable\n", "* Row vs Column Oriented\n", "* Read vs Write performance\n", "* Appendable\n", "* Splittable\n", "* Metadata storage\n", "\n", "*For additional information see Definitive Guide, Chapter 9: Data Sources, pg.153*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## First let's understand our data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir data\n", "!wget -q -O data/gsod-000000000000.csv.gz gs://w261-bucket/gsod/gsod-000000000000.csv.gz | head"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we see that we have several compressed CSV files as we expect based on our bq command specifying compressions. BigQuery was nice enough to split the files into 30 MB chunks so that our analysis will be partitioned nicely for ingestion.\n", "\n", "Now let's try to ingest these CSV's without any special commands or unzipped."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv = spark.read.option(\"header\", \"true\").csv(\"gs://w261-bucket/gsod/gsod-*.csv.gz\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "print((data_csv.count(), len(data_csv.columns)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wow that's nice we didn't even have to handle the decompression and it saves a ton on disk space! Next we're going to save this in a few different serializations so that we can see the effect on disk space.\n", "\n", "Also notice that since we have 114 million observations and 31 columns we should see some huge performance boosts for compression in general and particularly columnar compression with parquet since it takes into account the data type to improve compression further. While row based compression will be less.\n", "\n", "_Which Data Serialization do you think will do best?_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do these look?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have 4 data types below\n", "\n", "- Compressed CSV\n", "- Parquet\n", "- Avro\n", "- CSV\n", "\n", "Of these 3 are row oriented and 1 is column oriented. We have over 100M rows and 31 columns. Columnar compression should do fairly well in this scenerio. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Our original Compressed data already exists\n", "!gsutil du -sh gs://w261-bucket/gsod/gsod*.csv.gz"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv.write.format(\"parquet\").save(\"gs://w261-bucket/gsod/data.parquet\")\n", "!gsutil du -sh gs://w261-bucket/gsod/data.parquet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv.write.format(\"com.databricks.spark.avro\").save(\"gs://w261-bucket/gsod/data.avro\")\n", "!gsutil du -sh gs://w261-bucket/gsod/data.avro"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_csv.write.format(\"com.databricks.spark.csv\").save('gs://w261-bucket/gsod/data.csv')\n", "!gsutil du -sh gs://w261-bucket/gsod/data.csv"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How do these compare for simple computations?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First we need to read in the data again to ensure we're working with non-cached versions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_parquet = spark.read.parquet(\"gs://w261-bucket/gsod/data.parquet\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Count: \n", "Parquet keeps metadata about the data in order to compute some calculations extremely quickly such as row counts"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "data_parquet.count()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "data_csv.count()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Average of a column: \n", "Parquet is column oriented so it can go through the sequence of data in one step instead of taking each row. This should have much higher performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pyspark.sql import functions as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "data_parquet.agg(F.avg(data_parquet.max_temperature)).collect()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "data_csv.agg(F.avg(data.max_temperature)).collect()"]}, {"cell_type": "markdown", "metadata": {}, "source": [" > __DISCUSSION QUESTIONS:__ For each key term from the reading, briefly explain what it means in the context of this demo code. Specifically:\n", " * _What is the compression ratio for the parquet to csv file?_\n", " * _Which serialization would query a column faster?_\n", " * _Which types of columns do you think has the best compression for parquet?_\n", " * _When should you use flat files vs other data formats?_\n", " * _If we want to do analysis with lots of aggregations what serialization should we use?_\n", " * _Is there any downside to Parquet?_\n", " * _If you had to partition data into days as new data comes in with aggregations happening at end of day how would you operationalize this?_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 3. Working with DataFrames and simple User-Defined Functions (UDFs)\n", "\n", "In this example we're going to do some simple analysis of our data using built in spark functions. We'll look into UDFs and use a few instances of them to process our data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Using built-in Spark functions are always more efficient\n", "from pyspark.sql import types\n", "import pyspark.sql.functions as F\n", "\n", "timed = data_parquet.withColumn(\"time\", F.concat(F.col(\"year\"), F.lit(\"-\"), F.col(\"month\"), F.lit(\"-\"), F.col(\"day\")) \\\n", "                                .cast(types.TimestampType()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "timed.select('time').show(5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "timed.select('time').take(1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# A simple UDF for converting year, month, day to timestamps\n", "def create_date_from_parts(year, month, day):\n", "    return f'{year}-{month}-{day}'\n", "\n", "create_date_udf = F.udf(create_date_from_parts, types.StringType())\n", "timed_udf = data_parquet.withColumn(\"date\", create_date_udf('year', 'month', 'day').cast(types.TimestampType()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "timed_udf.take(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There's many things we could do from here but there are some important performance considerations when using UDFs. \n", "\n", "\n", "> User-defined functions and user-defined aggregate functions provide you with ways to extend the DataFrame and SQL APIs with your own custom code while keeping the Catalyst optimizer. The Dataset API (see \u201cDatasets\u201d on page 62) is another performant option for much of what you can do with UDFs and UDAFs. This is quite useful for performance, since otherwise you would need to convert the data to an RDD (and potentially back again) to perform arbitrary functions, which is quite expensive. (HP Spark pg 66) \n", "\n", "\n", "UDFs are typically much slower than built-in Spark functionality. The reason for this is becauase they have to serialize and deserialize the data for every row that the function is applied to. There have been recent improvements to UDF for some analytical results with Pandas UDFs that return scalars or groupby maps. Some more information about why UDFs are inefficent can be found here https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/\n", "\n", "Pandas UDFs solve the serialization issue by vectorizing the inputs and outputs, decreasing the serialziation from 3-100x; however, it isn't a golden bullet. See this blog for details http://garrens.com/blog/2018/03/04/using-new-pyspark-2-3-vectorized-pandas-udfs-lessons/\n", "\n", "See also: http://sparklingpandas.com/"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.display import Image\n", "from IPython.core.display import HTML \n", "Image(url= \"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\")"]}, {"cell_type": "markdown", "metadata": {}, "source": [">__DISCUSSION QUESTION:__ \n", "* What is the task here? What did we really accomplish?\n", "* What type does the UDF create_date_from_parts return?\n", "* What information is being stored in the data frame? Is  there anything inefficient about this data structure? \n", "* What types of situations would lead to an inefficeint data structure in DataFrames? Could we be more efficient using an RDD in those situations?\n", "* What questions would you ask of this table?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#  Exercise 4. EDA\n", "\n", "In this exercise we'll do some basic EDA/Sanity checks of our DataFrame, and start preparing it for our analysis in exercise 5."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["timed.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stations = spark.read.option(\"header\", \"true\").csv(\"gs://w261-bucket/gsod/stations.csv.gz\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stations.show(5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's filter for just the US since this is a US based dataset\n", "stations_us = stations.filter(F.col('Country')=='US')\n", "\n", "'''\n", "# Equivalently, we could write:\n", "stations_us = stations.where('Country'=='US')\n", "'''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are two methods to perform this operation: you can use `where` or `filter` (as above) and they will both perform the same operation. (pg 74 - Spark, The Definitive Guide). Remember that the DataFrame API and Spark SQL compile to the same execution plan.\n", "\n", "Take a look at the explain plan for pushdown predicates: (`PushedFilters: [IsNotNull(station_number)]`). (More info on pg 169, 325 - Spark, The Definitive Guide)  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# We need to bring that back to our timed dataframe\n", "timed_stations = timed.join(F.broadcast(stations_us), stations_us.usaf==timed.station_number, 'inner')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["timed_stations.explain()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Spark will automatically broadcast a small table, it's usually best to let Spark decide. See explain plan below. Notice that Spark broadcast the join for us even though we took that function out in our code. (p.151 Defintive Guide)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["timed_stations_NB = timed.join(stations_us, stations_us.usaf==timed.station_number, 'inner')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["timed_stations_NB.explain()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's only keep what we care about so we minimize our pain\n", "keep_columns = ['station_number', 'mean_temp', 'time', 'lat', 'lon']\n", "temp = timed_stations.select(*keep_columns)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's recast types\n", "temp = temp.withColumn(\"mean_temp\", temp[\"mean_temp\"].cast(types.DoubleType()))\n", "temp = temp.withColumn(\"lat\", temp[\"lat\"].cast(types.DoubleType()))\n", "temp = temp.withColumn(\"lon\", temp[\"lon\"].cast(types.DoubleType()))\n", "temp = temp.withColumn(\"station_number\", temp[\"station_number\"].cast(types.IntegerType()))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "# How is our dataframe looking? We did filter a bunch of data\n", "temp.describe().show()\n", "# You could also use 'summary()' to extract individual numbers for future use."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's look at some of the data in histograms\n", "# def plot_hist(hist_list):\n", "#     pd.DataFrame(\n", "#         list(zip(*hist_list)), \n", "#         columns=['bin', 'frequency']\n", "#     ).set_index(\n", "#         'bin'\n", "#     ).plot(kind='bar');"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_hist(labels,values):\n", "    df = pd.DataFrame({'lab':labels, 'val':values})\n", "    df.plot.bar(x='lab', y='val', rot=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "temp.cache()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# We will use filter instead of rdd, and take advantage of predicate pushdown\n", "import math\n", "def makeHistogram(_min,_max,numBuckets,colName):\n", "    _range = list(range(math.floor(_min), math.ceil(_max), round((abs(_min)+abs(_max))/numBuckets)))\n", "    _counts = np.zeros(len(_range))\n", "    for idx, val in enumerate(_range):\n", "        if idx < len(_range)-1:\n", "            _counts[idx] = temp.filter(F.col(colName) >= _range[idx]) \\\n", "                               .filter(F.col(colName) <= _range[idx+1]) \\\n", "                               .count()\n", "    plot_hist(_range,_counts)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "makeHistogram(-69.0,110.0,11,'mean_temp')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "makeHistogram(-179.63,179.583,11,'lon')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "makeHistogram(-60.483,80.13,11,'lat')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Using the RDD method (below) took almost an hour to run for a single plot. Even after caching the temp DF. Bummer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# %%time\n", "# temp_hist = temp.select('mean_temp').rdd.flatMap(lambda x: x).histogram(11)\n", "# plot_hist(temp_hist)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"../screenshots/lost-proxy.png\">"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# %%time\n", "# temp_hist = temp.select('lon').rdd.flatMap(lambda x: x).histogram(11)\n", "\n", "# # Loading the Computed Histogram into a Pandas Dataframe for plotting\n", "# plot_hist(temp_hist)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# %%time\n", "# temp_hist = temp.select('lat').rdd.flatMap(lambda x: x).histogram(11)\n", "\n", "# # Loading the Computed Histogram into a Pandas Dataframe for plotting\n", "# plot_hist(temp_hist)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 5 - Analysis\n", "\n", "In this section we're going to perform some aggregations to answer the question of \"Is there global warming?\" Don't take it too seriously, it's just an exercise! We're going to prepare our data so that we can draw an interactive graph displaying the change in avarage temperatures over time, as a function of latitude.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# let's use a struct to build a composite key\n", "temp = temp.withColumn('time-lat', F.struct('time','lat'))\n", "daily_average_at_latitude = temp.select('time-lat','mean_temp').groupBy(\"time-lat\").agg(F.avg('mean_temp'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you\u2019re used to RDDs you might be concerned by groupBy, but it is now a safe operation on thanks to the Spark SQL DataFrames optimizer, which automatically pipelines our reductions, avoiding giant shuffles and mega records. (HP Spark, pg. 43)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["daily_average_at_latitude.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# this function is here to make it easier to reason with the column names, flattens structs\n", "def flatten_df(nested_df):\n", "    flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n", "    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n", "\n", "    flat_df = nested_df.select(flat_cols +\n", "                               [F.col(nc+'.'+c).alias(nc+'_'+c)\n", "                                for nc in nested_cols\n", "                                for c in nested_df.select(nc+'.*').columns])\n", "    return flat_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["daily_average_at_latitude = flatten_df(daily_average_at_latitude)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["daily_average_at_latitude.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Now let's get the average on each latitude\n", "daily_average_at_latitude = daily_average_at_latitude.withColumn('time-rounded-lat', F.struct('time-lat_time',F.round(daily_average_at_latitude['time-lat_lat'],0)))\n", "average_by_lat = daily_average_at_latitude.groupby('time-rounded-lat').agg(F.avg('avg(mean_temp)'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# we used the struct in order to do a simple groupby that we can flatten again to get information\n", "average_by_lat.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# the names weren't very descriptive, let's rework that\n", "average_by_lat = flatten_df(average_by_lat)\n", "average_by_lat = average_by_lat.withColumnRenamed('time-rounded-lat_col2', 'rounded-lat')\n", "average_by_lat = average_by_lat.withColumnRenamed('time-rounded-lat_time-lat_time', 'time')\n", "average_by_lat = average_by_lat.withColumnRenamed('avg(avg(mean_temp))', 'temp')\n", "average_by_lat.printSchema()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# oh man that's a lot of stuff and since we can't cache the data this is taking forever to run.\n", "average_by_lat.explain()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Let's output to file and then read from that file to reduce our load.\n", "average_by_lat.write.format(\"parquet\").save(\"gs://w261-bucket/gsod/average_by_lat.parquet\")\n", "average_by_lat_read = spark.read.parquet(\"gs://w261-bucket/gsod/average_by_lat.parquet\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# I didn't time this, but it took about 17 minutes to save this to parquet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["average_by_lat_read.explain()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["average_by_lat_read.show(10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Small enough to fit in pandas for our final analysis. Let's do that\n", "average_by_lat_read.count()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = average_by_lat_read.toPandas()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.set_index(['rounded-lat','time'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.sort_index()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lat_list = df.index.levels[0]\n", "print(lat_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Is temperature increasing? Data isn't very clean and we didn't perform any sensor corrections.\n", "%matplotlib notebook\n", "from ipywidgets import *\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "n_roll = 365\n", "def f(x):\n", "    df.loc[x].rolling(n_roll).mean().plot()\n", "\n", "interact(f, x=lat_list);\n", "\n", "## Below is just a screenshot of the chart. To see the ineractive chart, follow the instructions below"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"../screenshots/plot-screenshot.png\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> __DISCUSSION QUESTIONS:__\n", "* Why did we create a struct for our groupBy?\n", "* Why did we push our transformations to a file and load them again? - compare and contrast cache(), save to disk (checkpoint), saveManagedTable\n", "* Where could we have done this before to save computation time?\n", "* Why did we do a rolling average of temperature?\n", "* Isn't pandas a lot easier to use?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# lets save this df for later plotting, just in case. We're still in the cloud here.\n", "df.to_csv(\"pandas.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# now download the file to local machine\n", "!gcloud compute scp w261-demo-m:/pandas.csv ~/MyDocuments/UCB/w261/Instructors/LiveSessionMaterials/wk08Demo_DataFrames/data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# SKIP THIS unless you want to render the chart while working locally:\n", "IMPORTANT: We are now on the local machine. Jupyter lab does not support matplotlib notebook. To view the chart, open in jupyter notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"pandas.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.set_index(['rounded-lat','time'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.sort_index()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lat_list = df.index.levels[0]\n", "print(lat_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Is temperature increasing? Data isn't very clean and we didn't perform any sensor corrections.\n", "%matplotlib notebook\n", "#%matplotlib inline\n", "from ipywidgets import *\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "n_roll = 365\n", "def f(x):\n", "    df.loc[x].rolling(n_roll).mean().plot()\n", "\n", "interact(f, x=lat_list);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "462px", "width": "252px"}, "navigate_menu": true, "number_sections": true, "sideBar": true, "threshold": 4, "toc_cell": true, "toc_position": {"height": "567px", "left": "0px", "right": "707.4456787109375px", "top": "105px", "width": "243px"}, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 2}